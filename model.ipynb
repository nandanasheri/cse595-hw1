{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fd2241",
   "metadata": {},
   "source": [
    "## CSE595 Homework 1 : Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f85f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "326ced81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\owner\\anaconda3\\envs\\cs421\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.8.0-cp310-cp310-win_amd64.whl (241.4 MB)\n",
      "   ---------------------------------------- 0.0/241.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/241.4 MB 15.2 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 6.6/241.4 MB 16.1 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 9.7/241.4 MB 15.9 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 12.6/241.4 MB 14.9 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 15.7/241.4 MB 15.0 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 18.9/241.4 MB 14.9 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 22.8/241.4 MB 15.3 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 26.5/241.4 MB 15.7 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 30.1/241.4 MB 15.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 33.8/241.4 MB 16.0 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 37.7/241.4 MB 16.3 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 42.2/241.4 MB 16.6 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 45.9/241.4 MB 16.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 49.8/241.4 MB 16.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 53.7/241.4 MB 16.9 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 57.1/241.4 MB 16.9 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 60.6/241.4 MB 16.8 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 64.0/241.4 MB 16.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 67.9/241.4 MB 16.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 72.1/241.4 MB 17.0 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 75.8/241.4 MB 17.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 78.6/241.4 MB 16.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 82.6/241.4 MB 16.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 86.2/241.4 MB 16.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 89.4/241.4 MB 16.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 92.5/241.4 MB 16.7 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 95.7/241.4 MB 16.6 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 99.1/241.4 MB 16.6 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 102.5/241.4 MB 16.6 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 105.6/241.4 MB 16.5 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 109.1/241.4 MB 16.5 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 111.9/241.4 MB 16.4 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 114.6/241.4 MB 16.3 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 116.4/241.4 MB 16.1 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 119.8/241.4 MB 16.0 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 123.2/241.4 MB 16.0 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 126.6/241.4 MB 16.0 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 129.8/241.4 MB 16.0 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 132.4/241.4 MB 15.9 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 135.3/241.4 MB 15.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 137.9/241.4 MB 15.7 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 141.3/241.4 MB 15.7 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 145.2/241.4 MB 15.8 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 149.2/241.4 MB 15.9 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 152.8/241.4 MB 15.9 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 156.8/241.4 MB 16.0 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 160.4/241.4 MB 16.0 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 164.4/241.4 MB 16.0 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 167.8/241.4 MB 16.0 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 172.0/241.4 MB 16.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 175.9/241.4 MB 16.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 178.5/241.4 MB 16.1 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 181.1/241.4 MB 16.0 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 183.8/241.4 MB 15.9 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 186.6/241.4 MB 15.9 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 188.2/241.4 MB 15.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 191.4/241.4 MB 15.7 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 194.5/241.4 MB 15.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 197.7/241.4 MB 15.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 201.6/241.4 MB 15.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 205.5/241.4 MB 15.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 209.5/241.4 MB 15.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 213.4/241.4 MB 15.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 217.3/241.4 MB 15.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 221.8/241.4 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 225.7/241.4 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 229.9/241.4 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 234.1/241.4 MB 16.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  238.6/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 241.4/241.4 MB 15.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 4.2/6.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f240b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stop words from NLTK Github Repo to be removed \n",
    "def load_stopwords():\n",
    "    f = open(\"stopwords.txt\")\n",
    "    words = f.readlines()\n",
    "    stopwords = set()\n",
    "    for i in words:\n",
    "        i = i.strip()\n",
    "        stopwords.add(i)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156ff00",
   "metadata": {},
   "source": [
    "### Part 1 : Representing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7af0dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes purely by using whitespace\n",
    "def tokenize (sentence : str):\n",
    "    tokens = sentence.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04b2a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing by ignoring capitalization, removing stop words as well as all punctuations\n",
    "def better_tokenize(text : str):\n",
    "    # store unique tokens in tokens\n",
    "    tokens = []\n",
    "    # convert entire text into lowercase\n",
    "    lowercase = text.lower()\n",
    "    # find all possible words and all possible punctuations as their own tokens \n",
    "    words = re.findall(r\"\\w+|[,.!?#\\r\\n$%;:()\\\"']\", lowercase)\n",
    "    # load in stop words\n",
    "    stopwords = load_stopwords()\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d42482d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary with a minimum word frequency of 250 along with a column mapping\n",
    "def build_vocab(token_docs: list):\n",
    "    vocab, all_tokens = [], []\n",
    "    vocab_indices = {}\n",
    "    for doc in token_docs:\n",
    "        all_tokens.extend(doc)\n",
    "\n",
    "    # returns frequences of tokens across all documents\n",
    "    frequencies = Counter(all_tokens)\n",
    "    \n",
    "    i = 0\n",
    "    for freq in frequencies:\n",
    "    #   if above certain threshold, add to vocab\n",
    "        # if frequencies[freq] >= 250:\n",
    "        vocab.append(freq)\n",
    "        vocab_indices[freq] = i\n",
    "        i += 1\n",
    "     \n",
    "    return set(vocab), vocab_indices    \n",
    "\n",
    "# create term document matrix\n",
    "def create_sparse_matrix(documents : list):\n",
    "    tokenized_docs = []\n",
    "    for doc in documents:\n",
    "        tokens = better_tokenize(doc)\n",
    "        tokenized_docs.append(tokens)\n",
    "    vocab, indices = build_vocab(tokenized_docs)\n",
    "\n",
    "    dense_matrix = []\n",
    "    for doc in tokenized_docs:\n",
    "        row = [0] * len(vocab)\n",
    "        for token in doc:\n",
    "            if token in vocab:\n",
    "                row[indices[token]] += 1\n",
    "        dense_matrix.append(row)\n",
    "    np_arr = np.array(dense_matrix)\n",
    "    sparse_td_matrix = sparse.csr_matrix(np_arr)\n",
    "    \n",
    "    return sparse_td_matrix, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078fa42b",
   "metadata": {},
   "source": [
    "#### Testing Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7596106b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'conjuring',\n",
       " '\"',\n",
       " 'one',\n",
       " 'thrilling',\n",
       " 'horror',\n",
       " 'film',\n",
       " 'come',\n",
       " 'recent',\n",
       " 'times',\n",
       " '.',\n",
       " '\"',\n",
       " 'sinister',\n",
       " '\"',\n",
       " 'one',\n",
       " 'scariest',\n",
       " 'films',\n",
       " 'watched',\n",
       " '.',\n",
       " 'since',\n",
       " \"'\",\n",
       " 'based',\n",
       " '\"',\n",
       " 'true',\n",
       " 'events',\n",
       " '\"',\n",
       " ',',\n",
       " 'makes',\n",
       " 'even',\n",
       " 'interesting',\n",
       " '.',\n",
       " \"'\",\n",
       " 'quite',\n",
       " 'give',\n",
       " 'full',\n",
       " '10',\n",
       " 'star',\n",
       " 'rating',\n",
       " 'quite',\n",
       " 'jump',\n",
       " 'scares',\n",
       " ',',\n",
       " 'least',\n",
       " 'good',\n",
       " 'portion',\n",
       " 'jump',\n",
       " 'scares',\n",
       " 'add',\n",
       " 'atmosphere',\n",
       " 'movie',\n",
       " '.',\n",
       " 'first',\n",
       " 'time',\n",
       " 'watcher',\n",
       " 'movie',\n",
       " ',',\n",
       " 'certainly',\n",
       " 'disturb',\n",
       " '.',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 'stick',\n",
       " 'like',\n",
       " '\"',\n",
       " 'sinister',\n",
       " '\"',\n",
       " 'long',\n",
       " ',',\n",
       " 'conjuring',\n",
       " 'fun',\n",
       " 'watch',\n",
       " 'always',\n",
       " 'love',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'rewatch',\n",
       " 'every',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"train.csv\")\n",
    "better_tokenize(df['generation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "640e810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 0 stored elements and shape (3, 0)>\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "documents_test = [\"I like the cat!\", \"the cat eats.\", \"i saw this cat and it was adorable i like it so much\"]\n",
    "matrix, indices = create_sparse_matrix(documents_test)\n",
    "\n",
    "print(matrix)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699094c0",
   "metadata": {},
   "source": [
    "### Part 2 : Logistic Regression w numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ccc3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sigmoid function that takes in a np array/vector\n",
    "def sigmoid(x):\n",
    "    result = 1/(1+np.exp(-x))\n",
    "    return result\n",
    "\n",
    "# weights is our beta coefficients, x is the feature vector of a document and y is the ground truth of a document\n",
    "def log_likelihood(x, y, weights):\n",
    "    a = y * weights.transpose() * x\n",
    "    b = np.log(1 + np.exp(weights.transpose() * x))\n",
    "    return a - b\n",
    "\n",
    "# compute the gradient for a specific x vector, beta coefficients and ground truth y\n",
    "def compute_gradient(x, y, weights):\n",
    "    return (sigmoid(weights * x) - y) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with X matrix each row is a feature vector, Y vector, rate of learning and number of steps\n",
    "def logistic_regression(X, Y, learning_rate, num_step):\n",
    "    weights = [0] * len(Y)\n",
    "    for step in num_step:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e08c6",
   "metadata": {},
   "source": [
    "### Part 3 : Logistic Regression w Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c8f9fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in sparse scipy matrix and converts it into a sparse tensor\n",
    "def to_sparse_tensor(sparse_scipy):\n",
    "    # return row and col indices of non zero values along with the values\n",
    "    row_ind, col_ind, values = sparse.find(sparse_scipy)\n",
    "\n",
    "    # merge both np arrays to a singular array - this is more efficient - ASK\n",
    "    indices = np.stack((row_ind, col_ind))\n",
    "    i = torch.tensor(indices)\n",
    "    v = torch.tensor(values, dtype=torch.float32)\n",
    "    return torch.sparse_coo_tensor(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05203b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Class extending nn.Module\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        # call the base class initialization\n",
    "        super().__init__()\n",
    "        # singular output feature for binary classification - is it LLM generated or not?\n",
    "        # input features would be the size of the vocabulary - how many features should we train on?\n",
    "        self.linear = nn.Linear(in_features = vocab_size, out_features=1)\n",
    "\n",
    "    # forward takes in input sparse matrix - documents with all features\n",
    "    def forward(self, x):\n",
    "        # pass the linear layer output into sigmoid function and return the probability\n",
    "        x = nn.Sigmoid(self.linear(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71a2cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 11 stored elements and shape (3, 8)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2],\n",
      "                       [0, 1, 2, 1, 3, 4, 0, 1, 5, 6, 7]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(3, 8), nnz=11, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "documents_test = [\"I like the cat!\", \"the cat eats.\", \"i saw this cat and it was adorable i like it so much\"]\n",
    "matrix, indices = create_sparse_matrix(documents_test)\n",
    "\n",
    "print(matrix)\n",
    "to_sparse_tensor(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
