{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fd2241",
   "metadata": {},
   "source": [
    "## CSE595 Homework 1 : Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f85f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "326ced81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\owner\\anaconda3\\envs\\cs421\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.8.0-cp310-cp310-win_amd64.whl (241.4 MB)\n",
      "   ---------------------------------------- 0.0/241.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/241.4 MB 15.2 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 6.6/241.4 MB 16.1 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 9.7/241.4 MB 15.9 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 12.6/241.4 MB 14.9 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 15.7/241.4 MB 15.0 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 18.9/241.4 MB 14.9 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 22.8/241.4 MB 15.3 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 26.5/241.4 MB 15.7 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 30.1/241.4 MB 15.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 33.8/241.4 MB 16.0 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 37.7/241.4 MB 16.3 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 42.2/241.4 MB 16.6 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 45.9/241.4 MB 16.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 49.8/241.4 MB 16.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 53.7/241.4 MB 16.9 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 57.1/241.4 MB 16.9 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 60.6/241.4 MB 16.8 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 64.0/241.4 MB 16.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 67.9/241.4 MB 16.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 72.1/241.4 MB 17.0 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 75.8/241.4 MB 17.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 78.6/241.4 MB 16.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 82.6/241.4 MB 16.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 86.2/241.4 MB 16.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 89.4/241.4 MB 16.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 92.5/241.4 MB 16.7 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 95.7/241.4 MB 16.6 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 99.1/241.4 MB 16.6 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 102.5/241.4 MB 16.6 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 105.6/241.4 MB 16.5 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 109.1/241.4 MB 16.5 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 111.9/241.4 MB 16.4 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 114.6/241.4 MB 16.3 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 116.4/241.4 MB 16.1 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 119.8/241.4 MB 16.0 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 123.2/241.4 MB 16.0 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 126.6/241.4 MB 16.0 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 129.8/241.4 MB 16.0 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 132.4/241.4 MB 15.9 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 135.3/241.4 MB 15.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 137.9/241.4 MB 15.7 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 141.3/241.4 MB 15.7 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 145.2/241.4 MB 15.8 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 149.2/241.4 MB 15.9 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 152.8/241.4 MB 15.9 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 156.8/241.4 MB 16.0 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 160.4/241.4 MB 16.0 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 164.4/241.4 MB 16.0 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 167.8/241.4 MB 16.0 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 172.0/241.4 MB 16.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 175.9/241.4 MB 16.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 178.5/241.4 MB 16.1 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 181.1/241.4 MB 16.0 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 183.8/241.4 MB 15.9 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 186.6/241.4 MB 15.9 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 188.2/241.4 MB 15.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 191.4/241.4 MB 15.7 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 194.5/241.4 MB 15.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 197.7/241.4 MB 15.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 201.6/241.4 MB 15.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 205.5/241.4 MB 15.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 209.5/241.4 MB 15.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 213.4/241.4 MB 15.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 217.3/241.4 MB 15.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 221.8/241.4 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 225.7/241.4 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 229.9/241.4 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 234.1/241.4 MB 16.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  238.6/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.4 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 241.4/241.4 MB 15.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 4.2/6.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f240b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stop words from NLTK Github Repo to be removed \n",
    "def load_stopwords():\n",
    "    f = open(\"stopwords.txt\")\n",
    "    words = f.readlines()\n",
    "    stopwords = set()\n",
    "    for i in words:\n",
    "        i = i.strip()\n",
    "        stopwords.add(i)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156ff00",
   "metadata": {},
   "source": [
    "### Part 1 : Representing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af0dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes purely by using whitespace\n",
    "def tokenize (sentence : str):\n",
    "    tokens = sentence.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b2a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing by ignoring capitalization, removing stop words as well as all punctuations\n",
    "def better_tokenize(text : str):\n",
    "    # store unique tokens in tokens\n",
    "    tokens = []\n",
    "    # convert entire text into lowercase\n",
    "    lowercase = text.lower()\n",
    "    # find all possible words and all possible punctuations as their own tokens \n",
    "    words = re.findall(r\"\\w+|[,.!?#\\r\\n$%;:()\\\"']\", lowercase)\n",
    "    # load in stop words\n",
    "    stopwords = load_stopwords()\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42482d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary with a minimum word frequency of 250 along with a column mapping\n",
    "def build_vocab(token_docs: list):\n",
    "    vocab, all_tokens = [], []\n",
    "    vocab_indices = {}\n",
    "    for doc in token_docs:\n",
    "        all_tokens.extend(doc)\n",
    "\n",
    "    # returns frequences of tokens across all documents\n",
    "    frequencies = Counter(all_tokens)\n",
    "    \n",
    "    i = 0\n",
    "    for freq in frequencies:\n",
    "    #   if above certain threshold, add to vocab\n",
    "        # if frequencies[freq] >= 250:\n",
    "        vocab.append(freq)\n",
    "        vocab_indices[freq] = i\n",
    "        i += 1\n",
    "     \n",
    "    return set(vocab), vocab_indices    \n",
    "\n",
    "# create term document matrix\n",
    "def create_sparse_matrix(documents : list):\n",
    "    tokenized_docs = []\n",
    "    for doc in documents:\n",
    "        tokens = better_tokenize(doc)\n",
    "        tokenized_docs.append(tokens)\n",
    "    vocab, indices = build_vocab(tokenized_docs)\n",
    "\n",
    "    dense_matrix = []\n",
    "    for doc in tokenized_docs:\n",
    "        row = [0] * len(vocab)\n",
    "        for token in doc:\n",
    "            if token in vocab:\n",
    "                row[indices[token]] += 1\n",
    "        dense_matrix.append(row)\n",
    "    np_arr = np.array(dense_matrix)\n",
    "    sparse_td_matrix = sparse.csr_matrix(np_arr)\n",
    "    \n",
    "    return sparse_td_matrix, indices\n",
    "\n",
    "# create a vector from a singular document of text\n",
    "def create_document_vector(text, indices):\n",
    "    tokens = better_tokenize(text)\n",
    "    row = [0] * len(indices)\n",
    "    for token in tokens:\n",
    "        if token in indices:\n",
    "            row[indices[token]] += 1\n",
    "    return np.array(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078fa42b",
   "metadata": {},
   "source": [
    "#### Testing Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7596106b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'conjuring',\n",
       " '\"',\n",
       " 'one',\n",
       " 'thrilling',\n",
       " 'horror',\n",
       " 'film',\n",
       " 'come',\n",
       " 'recent',\n",
       " 'times',\n",
       " '.',\n",
       " '\"',\n",
       " 'sinister',\n",
       " '\"',\n",
       " 'one',\n",
       " 'scariest',\n",
       " 'films',\n",
       " 'watched',\n",
       " '.',\n",
       " 'since',\n",
       " \"'\",\n",
       " 'based',\n",
       " '\"',\n",
       " 'true',\n",
       " 'events',\n",
       " '\"',\n",
       " ',',\n",
       " 'makes',\n",
       " 'even',\n",
       " 'interesting',\n",
       " '.',\n",
       " \"'\",\n",
       " 'quite',\n",
       " 'give',\n",
       " 'full',\n",
       " '10',\n",
       " 'star',\n",
       " 'rating',\n",
       " 'quite',\n",
       " 'jump',\n",
       " 'scares',\n",
       " ',',\n",
       " 'least',\n",
       " 'good',\n",
       " 'portion',\n",
       " 'jump',\n",
       " 'scares',\n",
       " 'add',\n",
       " 'atmosphere',\n",
       " 'movie',\n",
       " '.',\n",
       " 'first',\n",
       " 'time',\n",
       " 'watcher',\n",
       " 'movie',\n",
       " ',',\n",
       " 'certainly',\n",
       " 'disturb',\n",
       " '.',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 'stick',\n",
       " 'like',\n",
       " '\"',\n",
       " 'sinister',\n",
       " '\"',\n",
       " 'long',\n",
       " ',',\n",
       " 'conjuring',\n",
       " 'fun',\n",
       " 'watch',\n",
       " 'always',\n",
       " 'love',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'rewatch',\n",
       " 'every',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"train.csv\")\n",
    "better_tokenize(df['generation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640e810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 519 stored elements and shape (5, 481)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t8\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t6\n",
      "  (0, 10)\t2\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t3\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t4\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t2\n",
      "  (0, 24)\t1\n",
      "  :\t:\n",
      "  (4, 456)\t1\n",
      "  (4, 457)\t1\n",
      "  (4, 458)\t1\n",
      "  (4, 459)\t1\n",
      "  (4, 460)\t2\n",
      "  (4, 461)\t1\n",
      "  (4, 462)\t1\n",
      "  (4, 463)\t1\n",
      "  (4, 464)\t1\n",
      "  (4, 465)\t1\n",
      "  (4, 466)\t1\n",
      "  (4, 467)\t1\n",
      "  (4, 468)\t1\n",
      "  (4, 469)\t1\n",
      "  (4, 470)\t1\n",
      "  (4, 471)\t1\n",
      "  (4, 472)\t1\n",
      "  (4, 473)\t1\n",
      "  (4, 474)\t1\n",
      "  (4, 475)\t1\n",
      "  (4, 476)\t1\n",
      "  (4, 477)\t1\n",
      "  (4, 478)\t1\n",
      "  (4, 479)\t1\n",
      "  (4, 480)\t1\n",
      "{'\"': 0, 'conjuring': 1, 'one': 2, 'thrilling': 3, 'horror': 4, 'film': 5, 'come': 6, 'recent': 7, 'times': 8, '.': 9, 'sinister': 10, 'scariest': 11, 'films': 12, 'watched': 13, 'since': 14, \"'\": 15, 'based': 16, 'true': 17, 'events': 18, ',': 19, 'makes': 20, 'even': 21, 'interesting': 22, 'quite': 23, 'give': 24, 'full': 25, '10': 26, 'star': 27, 'rating': 28, 'jump': 29, 'scares': 30, 'least': 31, 'good': 32, 'portion': 33, 'add': 34, 'atmosphere': 35, 'movie': 36, 'first': 37, 'time': 38, 'watcher': 39, 'certainly': 40, 'disturb': 41, 'doesn': 42, 'stick': 43, 'like': 44, 'long': 45, 'fun': 46, 'watch': 47, 'always': 48, 'love': 49, 'coming': 50, 'back': 51, 'rewatch': 52, 'every': 53, 'majority': 54, 'owner': 55, 'embattled': 56, 'russian': 57, 'oil': 58, 'firm': 59, 'yukos': 60, 'sued': 61, 'government': 62, '$': 63, '28': 64, '3bn': 65, '(': 66, '15': 67, '2bn': 68, ')': 69, '\\n': 70, 'kremlin': 71, 'last': 72, 'year': 73, 'seized': 74, 'sold': 75, 'main': 76, 'production': 77, 'arm': 78, 'yugansk': 79, 'state': 80, 'run': 81, 'group': 82, 'rosneft': 83, '9': 84, 'offset': 85, 'massive': 86, 'tax': 87, 'bill': 88, 'menatep': 89, 'gibraltar': 90, 'holding': 91, 'company': 92, 'controls': 93, '64': 94, '%': 95, 'says': 96, 'illegal': 97, 'already': 98, 'asked': 99, 'repay': 100, '979m': 101, 'loan': 102, 'secured': 103, 'assets': 104, 'argument': 105, 'selling': 106, 'yuganskneftegaz': 107, 'unit': 108, 'name': 109, 'owed': 110, '27bn': 111, 'taxes': 112, 'years': 113, '7416': 114, 'onwards': 115, 'accused': 116, 'using': 117, 'web': 118, 'offshore': 119, 'firms': 120, 'avoid': 121, 'liabilities': 122, 'courts': 123, 'sent': 124, 'bailiffs': 125, 'freeze': 126, 'accounts': 127, 'seize': 128, 'critics': 129, 'say': 130, 'sell': 131, 'assault': 132, 'finances': 133, 'part': 134, 'attempt': 135, 'bring': 136, 'energy': 137, 'industry': 138, 'control': 139, 'according': 140, 'actions': 141, 'contrary': 142, '1994': 143, 'charter': 144, 'treaty': 145, 'designed': 146, 'regulate': 147, 'disagreements': 148, 'investments': 149, 'warned': 150, 'continuing': 151, 'attacks': 152, 'personnel': 153, 'shareholders': 154, 'buyer': 155, 'would': 156, 'face': 157, 'lifetime': 158, 'litigation': 159, 'said': 160, 'tim': 161, 'osborne': 162, 'director': 163, 'warning': 164, 'recover': 165, 'value': 166, 'losses': 167, 'begin': 168, 'earnest': 169, 'today': 170, 'shareholding': 171, 'gone': 172, '36': 173, '5bn': 174, 'virtually': 175, 'nothing': 176, '2003': 177, 'result': 178, 'action': 179, 'shares': 180, 'fallen': 181, '60': 182, 'paris': 183, 'lawyer': 184, 'emmanuel': 185, 'gaillard': 186, 'shearman': 187, 'sterling': 188, 'overall': 189, 'claim': 190, 'figure': 191, '45': 192, 'addition': 193, 'share': 194, 'gains': 195, 'could': 196, 'accrued': 197, 'arbitration': 198, 'lawsuit': 199, 'take': 200, 'place': 201, 'stockholm': 202, 'hague': 203, 'mr': 204, 'russia': 205, 'signed': 206, 'never': 207, 'ratified': 208, 'experts': 209, 'make': 210, 'difficult': 211, 'press': 212, 'case': 213, 'told': 214, 'bbc': 215, 'news': 216, 'came': 217, 'effect': 218, 'signature': 219, 'ratification': 220, 'past': 221, 'bound': 222, 'attract': 223, 'foreign': 224, 'investors': 225, 'still': 226, 'waiting': 227, 'see': 228, 'happen': 229, 'filing': 230, 'us': 231, 'court': 232, 'bankruptcy': 233, 'protection': 234, 'took': 235, 'try': 236, 'prevent': 237, 'forced': 238, 'sale': 239, 'little': 240, 'known': 241, 'shell': 242, 'turn': 243, 'bought': 244, 'claims': 245, 'downfall': 246, 'punishment': 247, 'political': 248, 'ambitions': 249, 'founder': 250, 'mikhail': 251, 'khodorkovsky': 252, 'currently': 253, 'facing': 254, 'fraud': 255, 'evasion': 256, 'charges': 257, 'founders': 258, 'fellow': 259, 'elissa': 260, 'panush': 261, 'benedek': 262, 'born': 263, 'september': 264, '1936': 265, 'american': 266, 'psychiatrist': 267, 'specializing': 268, 'child': 269, 'adolescent': 270, 'psychiatry': 271, 'forensic': 272, 'adjunct': 273, 'clinical': 274, 'professor': 275, 'university': 276, 'michigan': 277, 'medical': 278, 'center': 279, 'served': 280, 'research': 281, 'training': 282, 'ann': 283, 'arbor': 284, '25': 285, 'president': 286, 'psychiatric': 287, 'association': 288, '1990': 289, '1991': 290, 'regarded': 291, 'expert': 292, 'abuse': 293, 'trauma': 294, 'testified': 295, 'high': 296, 'profile': 297, 'cases': 298, 'also': 299, 'focuses': 300, 'ethics': 301, 'aspects': 302, 'disasters': 303, 'terrorism': 304, 'domestic': 305, 'violence': 306, 'books': 307, 'book': 308, 'chapters': 309, 'articles': 310, 'collaborated': 311, 'husband': 312, 'attorney': 313, 'richard': 314, 'studies': 315, 'divorce': 316, 'custody': 317, 'early': 318, 'life': 319, 'education': 320, 'detroit': 321, 'louis': 322, 'tillie': 323, 'father': 324, 'school': 325, 'principal': 326, 'science': 327, 'teacher': 328, 'mother': 329, 'taught': 330, 'elementary': 331, 'three': 332, 'sisters': 333, 'graduated': 334, 'central': 335, '1954': 336, 'won': 337, 'scholarship': 338, 'waterkeeper': 339, 'alliance': 340, 'worldwide': 341, 'network': 342, 'environmental': 343, 'organizations': 344, 'founded': 345, '1999': 346, 'response': 347, 'growing': 348, 'movement': 349, 'names': 350, 'riverkeeper': 351, 'baykeeper': 352, 'soundkeeper': 353, 'december': 354, '2019': 355, 'grown': 356, '571': 357, 'members': 358, '44': 359, 'countries': 360, 'half': 361, 'membership': 362, 'outside': 363, 'u': 364, ';': 365, 'added': 366, '050': 367, 'groups': 368, 'five': 369, 'original': 370, 'organized': 371, '1983': 372, 'started': 373, 'hudson': 374, 'river': 375, 'new': 376, 'york': 377, 'destructive': 378, 'industrial': 379, 'pollution': 380, 'destroying': 381, 'soon': 382, 'followed': 383, 'island': 384, 'led': 385, 'terry': 386, 'backer': 387, 'delaware': 388, 'san': 389, 'francisco': 390, 'jersey': 391, 'others': 392, 'manhattan': 393, 'unites': 394, 'coordinating': 395, 'covering': 396, 'issues': 397, 'affecting': 398, 'waterkeepers': 399, 'work': 400, 'protect': 401, 'rivers': 402, 'lakes': 403, 'bays': 404, 'sounds': 405, 'water': 406, 'bodies': 407, 'around': 408, 'world': 409, 'united': 410, 'states': 411, 'east': 412, 'coast': 413, 'strongly': 414, 'represented': 415, '52': 416, '993': 417, 'cover': 418, 'watersheds': 419, 'west': 420, 'mississippi': 421, 'academic': 422, 'paper': 423, 'presents': 424, 'introduction': 425, 'segment': 426, 'phrase': 427, 'table': 428, 'spt': 429, 'novel': 430, 'method': 431, 'semantic': 432, 'segmentation': 433, 'visual': 434, 'entailment': 435, 'paraphrasing': 436, 'tasks': 437, 'within': 438, 'field': 439, 'artificial': 440, 'intelligence': 441, 'phrases': 442, 'associated': 443, 'concepts': 444, 'connected': 445, 'respective': 446, 'image': 447, 'segments': 448, 'enhancing': 449, 'interpretability': 450, 'accuracy': 451, 'explores': 452, 'trained': 453, 'multiple': 454, 'instance': 455, 'learning': 456, 'subsequently': 457, 'utilised': 458, 'two': 459, 'applications': 460, ':': 461, 'predicting': 462, 'visually': 463, 'entails': 464, 'sentence': 465, 'descriptions': 466, 'experimental': 467, 'results': 468, 'suggest': 469, 'approach': 470, 'outperforms': 471, 'existing': 472, 'baselines': 473, 'substantially': 474, 'highlighting': 475, 'potential': 476, 'significance': 477, 'diverse': 478, 'language': 479, 'vision': 480}\n"
     ]
    }
   ],
   "source": [
    "documents_test = [\"I like the cat!\", \"the cat eats.\", \"i saw this cat and it was adorable i like it so much\"]\n",
    "matrix, indices = create_sparse_matrix(df[\"generation\"][:5])\n",
    "\n",
    "print(matrix)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699094c0",
   "metadata": {},
   "source": [
    "### Part 2 : Logistic Regression w numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ccc3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sigmoid function that takes in a np array/vector\n",
    "def sigmoid(x):\n",
    "    result = 1/(1+np.exp(-x))\n",
    "    return result\n",
    "\n",
    "# weights is our beta coefficients, x is the feature vector of a document and y is the ground truth of a document\n",
    "def log_likelihood(x, y, beta):\n",
    "    a = y * beta.transpose() * x\n",
    "    b = np.log(1 + np.exp(beta.transpose() * x))\n",
    "    return a - b\n",
    "\n",
    "# compute the gradient for a specific x vector, beta coefficients and ground truth y\n",
    "def compute_gradient(x, y, predicted_y):\n",
    "    return (sigmoid(predicted_y) - y) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with X matrix each row is a feature vector, Y vector, rate of learning and number of steps\n",
    "def logistic_regression(X, Y, learning_rate, num_step):\n",
    "    beta = [0] * len(Y)\n",
    "    # beta values - vector of coefficients initialized to zero\n",
    "    beta = np.array(beta)\n",
    "\n",
    "    # Adding a column of ones as bias vector\n",
    "    bias = np.array([1] * len(X)).reshape(-1, 1)\n",
    "    sparse_bias = sparse.csr_matrix(bias)\n",
    "    # horizontally stacking training matrix with bias vector\n",
    "    X = sparse.hstack([X, sparse_bias], format=\"csr\")\n",
    "\n",
    "    for i in range(num_step):\n",
    "        predicted_y = np.matmul(X.getrow(i), beta)\n",
    "        beta -= learning_rate * compute_gradient(X.getrow(i), Y[i], predicted_y)\n",
    "\n",
    "        print(\"Log Likelihood :\", log_likelihood(X.getrow(i), Y[i], beta))\n",
    "\n",
    "def predict(text, beta, indices):\n",
    "    test_features = create_document_vector(text, indices)\n",
    "    predicted_y = np.matmul(test_features, beta)\n",
    "    return predicted_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d11a1e",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f572d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m matrix, indices \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sparse_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(matrix)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices)\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mcreate_sparse_matrix\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m     29\u001b[0m dense_matrix \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tokenized_docs:\n\u001b[1;32m---> 31\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m vocab:\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv(\"train.csv\")\n",
    "\n",
    "matrix, indices = create_sparse_matrix(df[\"generation\"][:1000])\n",
    "\n",
    "print(matrix)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e08c6",
   "metadata": {},
   "source": [
    "### Part 3 : Logistic Regression w Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c8f9fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in sparse scipy matrix and converts it into a sparse tensor\n",
    "def to_sparse_tensor(sparse_scipy):\n",
    "    # return row and col indices of non zero values along with the values\n",
    "    row_ind, col_ind, values = sparse.find(sparse_scipy)\n",
    "\n",
    "    # merge both np arrays to a singular array - this is more efficient - ASK\n",
    "    indices = np.stack((row_ind, col_ind))\n",
    "    i = torch.tensor(indices)\n",
    "    v = torch.tensor(values, dtype=torch.float32)\n",
    "    return torch.sparse_coo_tensor(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05203b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Class extending nn.Module\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        # call the base class initialization\n",
    "        super().__init__()\n",
    "        # singular output feature for binary classification - is it LLM generated or not?\n",
    "        # input features would be the size of the vocabulary - how many features should we train on?\n",
    "        self.linear = nn.Linear(in_features = vocab_size, out_features=1)\n",
    "\n",
    "    # forward takes in input sparse matrix - documents with all features\n",
    "    def forward(self, x):\n",
    "        # pass the linear layer output into sigmoid function and return the probability\n",
    "        x = nn.Sigmoid(self.linear(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71a2cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 11 stored elements and shape (3, 8)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2],\n",
      "                       [0, 1, 2, 1, 3, 4, 0, 1, 5, 6, 7]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(3, 8), nnz=11, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "documents_test = [\"I like the cat!\", \"the cat eats.\", \"i saw this cat and it was adorable i like it so much\"]\n",
    "matrix, indices = create_sparse_matrix(documents_test)\n",
    "\n",
    "print(matrix)\n",
    "to_sparse_tensor(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
